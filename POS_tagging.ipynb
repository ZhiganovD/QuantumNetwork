{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Quantum-Enhanced LSTM Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting absl-py==0.12.0\n",
      "  Using cached absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Collecting aiohttp==3.7.4.post0\n",
      "  Using cached aiohttp-3.7.4.post0.tar.gz (1.1 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: appdirs==1.4.4 in /Users/dvzhiganov/miniforge3/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (1.4.4)\n",
      "Collecting appnope==0.1.2\n",
      "  Using cached appnope-0.1.2-py2.py3-none-any.whl (4.3 kB)\n",
      "Collecting argon2-cffi==21.1.0\n",
      "  Using cached argon2-cffi-21.1.0.tar.gz (1.8 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting async-timeout==3.0.1\n",
      "  Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
      "Collecting attrs==20.3.0\n",
      "  Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
      "Collecting autograd==1.3\n",
      "  Using cached autograd-1.3.tar.gz (38 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: backcall==0.2.0 in /Users/dvzhiganov/miniforge3/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (0.2.0)\n",
      "Collecting beautifulsoup4==4.9.3\n",
      "  Using cached beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\n",
      "Collecting bleach==4.1.0\n",
      "  Using cached bleach-4.1.0-py2.py3-none-any.whl (157 kB)\n",
      "Collecting bs4==0.0.1\n",
      "  Using cached bs4-0.0.1.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cachetools==4.2.1\n",
      "  Using cached cachetools-4.2.1-py3-none-any.whl (12 kB)\n",
      "Collecting certifi==2020.12.5\n",
      "  Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB)\n",
      "Collecting cffi==1.14.5\n",
      "  Using cached cffi-1.14.5.tar.gz (475 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting chardet==4.0.0\n",
      "  Using cached chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
      "Collecting click==7.1.2\n",
      "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting cryptography==3.4.7\n",
      "  Using cached cryptography-3.4.7-cp36-abi3-macosx_11_0_arm64.whl (1.9 MB)\n",
      "Collecting cycler==0.10.0\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Collecting debugpy==1.4.3\n",
      "  Using cached debugpy-1.4.3-py2.py3-none-any.whl (4.1 MB)\n",
      "Collecting decorator==4.4.2\n",
      "  Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Requirement already satisfied: defusedxml==0.7.1 in /Users/dvzhiganov/miniforge3/lib/python3.9/site-packages (from -r requirements.txt (line 22)) (0.7.1)\n",
      "Collecting dill==0.3.3\n",
      "  Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
      "Collecting dlx==1.0.4\n",
      "  Using cached dlx-1.0.4.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting docplex==2.15.194\n",
      "  Using cached docplex-2.15.194.tar.gz (582 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting entrypoints==0.3\n",
      "  Using cached entrypoints-0.3-py2.py3-none-any.whl (11 kB)\n",
      "Collecting fastdtw==0.3.4\n",
      "  Using cached fastdtw-0.3.4.tar.gz (133 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fastjsonschema==2.15.0\n",
      "  Using cached fastjsonschema-2.15.0-py3-none-any.whl (21 kB)\n",
      "Collecting fsspec==0.9.0\n",
      "  Using cached fsspec-0.9.0-py3-none-any.whl (107 kB)\n",
      "Requirement already satisfied: future==0.18.2 in /Users/dvzhiganov/miniforge3/lib/python3.9/site-packages (from -r requirements.txt (line 30)) (0.18.2)\n",
      "Collecting google-auth==1.28.0\n",
      "  Using cached google_auth-1.28.0-py2.py3-none-any.whl (136 kB)\n",
      "Collecting google-auth-oauthlib==0.4.4\n",
      "  Using cached google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Collecting grpcio==1.37.0\n",
      "  Using cached grpcio-1.37.0.tar.gz (21.7 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: h5py==3.1.0 in /Users/dvzhiganov/miniforge3/lib/python3.9/site-packages (from -r requirements.txt (line 34)) (3.1.0)\n",
      "Collecting idna==2.10\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting inflection==0.5.1\n",
      "  Using cached inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\n",
      "Collecting ipykernel==6.4.1\n",
      "  Using cached ipykernel-6.4.1-py3-none-any.whl (124 kB)\n",
      "Collecting ipython==7.28.0\n",
      "  Using cached ipython-7.28.0-py3-none-any.whl (788 kB)\n",
      "Requirement already satisfied: ipython-genutils==0.2.0 in /Users/dvzhiganov/miniforge3/lib/python3.9/site-packages (from -r requirements.txt (line 39)) (0.2.0)\n",
      "Collecting ipywidgets==7.6.5\n",
      "  Using cached ipywidgets-7.6.5-py2.py3-none-any.whl (121 kB)\n",
      "Collecting jedi==0.18.0\n",
      "  Using cached jedi-0.18.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting Jinja2==3.0.1\n",
      "  Using cached Jinja2-3.0.1-py3-none-any.whl (133 kB)\n",
      "Collecting joblib==1.0.1\n",
      "  Using cached joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "Collecting jsonschema==3.2.0\n",
      "  Using cached jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "Collecting jupyter==1.0.0\n",
      "  Using cached jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
      "Collecting jupyter-client==7.0.5\n",
      "  Using cached jupyter_client-7.0.5-py3-none-any.whl (124 kB)\n",
      "Collecting jupyter-console==6.4.0\n",
      "  Using cached jupyter_console-6.4.0-py3-none-any.whl (22 kB)\n",
      "Collecting jupyter-core==4.8.1\n",
      "  Using cached jupyter_core-4.8.1-py3-none-any.whl (86 kB)\n",
      "Collecting jupyterlab-pygments==0.1.2\n",
      "  Using cached jupyterlab_pygments-0.1.2-py2.py3-none-any.whl (4.6 kB)\n",
      "Collecting jupyterlab-widgets==1.0.2\n",
      "  Using cached jupyterlab_widgets-1.0.2-py3-none-any.whl (243 kB)\n",
      "Collecting kiwisolver==1.3.1\n",
      "  Using cached kiwisolver-1.3.1-cp39-cp39-macosx_11_0_arm64.whl (59 kB)\n",
      "Collecting lxml==4.6.3\n",
      "  Using cached lxml-4.6.3.tar.gz (3.2 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting Markdown==3.3.4\n",
      "  Using cached Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting MarkupSafe==2.0.1\n",
      "  Using cached MarkupSafe-2.0.1-cp39-cp39-macosx_10_9_universal2.whl (18 kB)\n",
      "Collecting matplotlib==3.4.1\n",
      "  Using cached matplotlib-3.4.1.tar.gz (37.3 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib-inline==0.1.3 in /Users/dvzhiganov/miniforge3/lib/python3.9/site-packages (from -r requirements.txt (line 56)) (0.1.3)\n",
      "Requirement already satisfied: mistune==0.8.4 in /Users/dvzhiganov/miniforge3/lib/python3.9/site-packages (from -r requirements.txt (line 57)) (0.8.4)\n",
      "Collecting more-itertools==8.7.0\n",
      "  Using cached more_itertools-8.7.0-py3-none-any.whl (48 kB)\n",
      "Collecting mpmath==1.2.1\n",
      "  Using cached mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
      "Collecting multidict==5.1.0\n",
      "  Using cached multidict-5.1.0.tar.gz (53 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting multitasking==0.0.9\n",
      "  Using cached multitasking-0.0.9.tar.gz (8.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting nbclient==0.5.4\n",
      "  Using cached nbclient-0.5.4-py3-none-any.whl (66 kB)\n",
      "Collecting nbconvert==6.2.0\n",
      "  Using cached nbconvert-6.2.0-py3-none-any.whl (553 kB)\n",
      "Collecting nbformat==5.1.3\n",
      "  Using cached nbformat-5.1.3-py3-none-any.whl (178 kB)\n",
      "Collecting nest-asyncio==1.5.1\n",
      "  Using cached nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB)\n",
      "Collecting networkx==2.5.1\n",
      "  Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB)\n",
      "Collecting nltk==3.6.1\n",
      "  Using cached nltk-3.6.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting notebook==6.4.4\n",
      "  Using cached notebook-6.4.4-py3-none-any.whl (9.9 MB)\n",
      "Collecting ntlm-auth==1.5.0\n",
      "  Using cached ntlm_auth-1.5.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting numpy==1.20.1\n",
      "  Using cached numpy-1.20.1.zip (7.8 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting oauthlib==3.1.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting packaging==21.0\n",
      "  Using cached packaging-21.0-py3-none-any.whl (40 kB)\n",
      "Collecting pandas==1.2.3\n",
      "  Using cached pandas-1.2.3.tar.gz (5.5 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandocfilters==1.5.0 in /Users/dvzhiganov/miniforge3/lib/python3.9/site-packages (from -r requirements.txt (line 74)) (1.5.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting parso==0.8.2\n",
      "  Using cached parso-0.8.2-py2.py3-none-any.whl (94 kB)\n",
      "Collecting PennyLane==0.14.1\n",
      "  Using cached PennyLane-0.14.1.tar.gz (404 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting PennyLane-qiskit==0.14.0\n",
      "  Using cached PennyLane_qiskit-0.14.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: pexpect==4.8.0 in /Users/dvzhiganov/miniforge3/lib/python3.9/site-packages (from -r requirements.txt (line 78)) (4.8.0)\n",
      "Requirement already satisfied: pickleshare==0.7.5 in /Users/dvzhiganov/miniforge3/lib/python3.9/site-packages (from -r requirements.txt (line 79)) (0.7.5)\n",
      "Collecting Pillow==8.2.0\n",
      "  Using cached Pillow-8.2.0-cp39-cp39-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Collecting ply==3.11\n",
      "  Using cached ply-3.11-py2.py3-none-any.whl (49 kB)\n",
      "Collecting prometheus-client==0.11.0\n",
      "  Using cached prometheus_client-0.11.0-py2.py3-none-any.whl (56 kB)\n",
      "Collecting prompt-toolkit==3.0.20\n",
      "  Using cached prompt_toolkit-3.0.20-py3-none-any.whl (370 kB)\n",
      "Collecting protobuf==3.15.7\n",
      "  Using cached protobuf-3.15.7-py2.py3-none-any.whl (173 kB)\n",
      "Collecting psutil==5.8.0\n",
      "  Using cached psutil-5.8.0.tar.gz (470 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ptyprocess==0.7.0 in /Users/dvzhiganov/miniforge3/lib/python3.9/site-packages (from -r requirements.txt (line 86)) (0.7.0)\n",
      "Requirement already satisfied: pyasn1==0.4.8 in /Users/dvzhiganov/miniforge3/lib/python3.9/site-packages (from -r requirements.txt (line 87)) (0.4.8)\n",
      "Collecting pyasn1-modules==0.2.8\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting pybind11==2.6.2\n",
      "  Using cached pybind11-2.6.2-py2.py3-none-any.whl (191 kB)\n",
      "Collecting pycparser==2.20\n",
      "  Using cached pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
      "Collecting Pygments==2.10.0\n",
      "  Using cached Pygments-2.10.0-py3-none-any.whl (1.0 MB)\n",
      "Collecting pyparsing==2.4.7\n",
      "  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Collecting pyrsistent==0.17.3\n",
      "  Using cached pyrsistent-0.17.3.tar.gz (106 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting python-constraint==1.4.0\n",
      "  Using cached python-constraint-1.4.0.tar.bz2 (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting python-dateutil==2.8.1\n",
      "  Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Collecting pytorch-lightning==1.2.7\n",
      "  Using cached pytorch_lightning-1.2.7-py3-none-any.whl (830 kB)\n",
      "Collecting pytz==2021.1\n",
      "  Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
      "Collecting PyYAML==5.3.1\n",
      "  Using cached PyYAML-5.3.1.tar.gz (269 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyzmq==22.3.0\n",
      "  Using cached pyzmq-22.3.0-cp39-cp39-macosx_10_15_universal2.whl (2.0 MB)\n",
      "Collecting qiskit==0.25.0\n",
      "  Using cached qiskit-0.25.0.tar.gz (4.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting qiskit-aer==0.8.0\n",
      "  Using cached qiskit-aer-0.8.0.tar.gz (6.5 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting qiskit-aqua==0.9.0\n",
      "  Using cached qiskit_aqua-0.9.0-py3-none-any.whl (2.1 MB)\n",
      "Collecting qiskit-ibmq-provider==0.12.2\n",
      "  Using cached qiskit_ibmq_provider-0.12.2-py3-none-any.whl (198 kB)\n",
      "Collecting qiskit-ignis==0.6.0\n",
      "  Using cached qiskit_ignis-0.6.0-py3-none-any.whl (207 kB)\n",
      "Collecting qiskit-terra==0.17.0\n",
      "  Using cached qiskit-terra-0.17.0.tar.gz (5.4 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[23 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/dvzhiganov/miniforge3/lib/python3.9/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 363, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/dvzhiganov/miniforge3/lib/python3.9/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 345, in main\n",
      "  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/dvzhiganov/miniforge3/lib/python3.9/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 130, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/7h/jxjph4114vl8_fbz1jq08t540000gn/T/pip-build-env-0y8r2rg0/overlay/lib/python3.9/site-packages/setuptools/build_meta.py\", line 177, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return self._get_build_requires(\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/7h/jxjph4114vl8_fbz1jq08t540000gn/T/pip-build-env-0y8r2rg0/overlay/lib/python3.9/site-packages/setuptools/build_meta.py\", line 159, in _get_build_requires\n",
      "  \u001b[31m   \u001b[0m     self.run_setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/7h/jxjph4114vl8_fbz1jq08t540000gn/T/pip-build-env-0y8r2rg0/overlay/lib/python3.9/site-packages/setuptools/build_meta.py\", line 281, in run_setup\n",
      "  \u001b[31m   \u001b[0m     super(_BuildMetaLegacyBackend,\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/7h/jxjph4114vl8_fbz1jq08t540000gn/T/pip-build-env-0y8r2rg0/overlay/lib/python3.9/site-packages/setuptools/build_meta.py\", line 174, in run_setup\n",
      "  \u001b[31m   \u001b[0m     exec(code, locals())\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 114, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/7h/jxjph4114vl8_fbz1jq08t540000gn/T/pip-build-env-0y8r2rg0/overlay/lib/python3.9/site-packages/Cython/Build/Dependencies.py\", line 970, in cythonize\n",
      "  \u001b[31m   \u001b[0m     module_list, module_metadata = create_extension_list(\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/7h/jxjph4114vl8_fbz1jq08t540000gn/T/pip-build-env-0y8r2rg0/overlay/lib/python3.9/site-packages/Cython/Build/Dependencies.py\", line 816, in create_extension_list\n",
      "  \u001b[31m   \u001b[0m     for file in nonempty(sorted(extended_iglob(filepattern)), \"'%s' doesn't match any files\" % filepattern):\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/7h/jxjph4114vl8_fbz1jq08t540000gn/T/pip-build-env-0y8r2rg0/overlay/lib/python3.9/site-packages/Cython/Build/Dependencies.py\", line 114, in nonempty\n",
      "  \u001b[31m   \u001b[0m     raise ValueError(error_msg)\n",
      "  \u001b[31m   \u001b[0m ValueError: 'qiskit/quantum_info/states/cython/exp_value.pyx' doesn't match any files\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pennylane as qml\n",
    "\n",
    "\n",
    "class QLSTM(nn.Module): #base LSTM structure for qubits\n",
    "    def __init__(self, \n",
    "                input_size, \n",
    "                hidden_size, \n",
    "                n_qubits=4,\n",
    "                n_qlayers=1,\n",
    "                batch_first=True,\n",
    "                return_sequences=False, \n",
    "                return_state=False,\n",
    "                backend=\"default.qubit\"):\n",
    "        super(QLSTM, self).__init__()\n",
    "        self.n_inputs = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.concat_size = self.n_inputs + self.hidden_size\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_qlayers = n_qlayers\n",
    "        self.backend = backend  # \"default.qubit\", \"qiskit.basicaer\", \"qiskit.ibm\"\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "        self.return_sequences = return_sequences\n",
    "        self.return_state = return_state\n",
    "\n",
    "        self.wires_forget = [f\"wire_forget_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_input = [f\"wire_input_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_update = [f\"wire_update_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_output = [f\"wire_output_{i}\" for i in range(self.n_qubits)]\n",
    "\n",
    "        self.dev_forget = qml.device(self.backend, wires=self.wires_forget)\n",
    "        self.dev_input = qml.device(self.backend, wires=self.wires_input)\n",
    "        self.dev_update = qml.device(self.backend, wires=self.wires_update)\n",
    "        self.dev_output = qml.device(self.backend, wires=self.wires_output)\n",
    "\n",
    "        def _circuit_forget(inputs, weights):\n",
    "            qml.templates.AngleEmbedding(inputs, wires=self.wires_forget)\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=self.wires_forget)\n",
    "            return [qml.expval(qml.PauliZ(wires=w)) for w in self.wires_forget]\n",
    "        self.qlayer_forget = qml.QNode(_circuit_forget, self.dev_forget, interface=\"torch\")\n",
    "\n",
    "        def _circuit_input(inputs, weights):\n",
    "            qml.templates.AngleEmbedding(inputs, wires=self.wires_input)\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=self.wires_input)\n",
    "            return [qml.expval(qml.PauliZ(wires=w)) for w in self.wires_input]\n",
    "        self.qlayer_input = qml.QNode(_circuit_input, self.dev_input, interface=\"torch\")\n",
    "\n",
    "        def _circuit_update(inputs, weights):\n",
    "            qml.templates.AngleEmbedding(inputs, wires=self.wires_update)\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=self.wires_update)\n",
    "            return [qml.expval(qml.PauliZ(wires=w)) for w in self.wires_update]\n",
    "        self.qlayer_update = qml.QNode(_circuit_update, self.dev_update, interface=\"torch\")\n",
    "\n",
    "        def _circuit_output(inputs, weights):\n",
    "            qml.templates.AngleEmbedding(inputs, wires=self.wires_output)\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=self.wires_output)\n",
    "            return [qml.expval(qml.PauliZ(wires=w)) for w in self.wires_output]\n",
    "        self.qlayer_output = qml.QNode(_circuit_output, self.dev_output, interface=\"torch\")\n",
    "\n",
    "        weight_shapes = {\"weights\": (n_qlayers, n_qubits)}\n",
    "        print(f\"weight_shapes = (n_qlayers, n_qubits) = ({n_qlayers}, {n_qubits})\")\n",
    "\n",
    "        self.clayer_in = torch.nn.Linear(self.concat_size, n_qubits)\n",
    "        self.VQC = {\n",
    "            'forget': qml.qnn.TorchLayer(self.qlayer_forget, weight_shapes),\n",
    "            'input': qml.qnn.TorchLayer(self.qlayer_input, weight_shapes),\n",
    "            'update': qml.qnn.TorchLayer(self.qlayer_update, weight_shapes),\n",
    "            'output': qml.qnn.TorchLayer(self.qlayer_output, weight_shapes)\n",
    "        }\n",
    "        self.clayer_out = torch.nn.Linear(self.n_qubits, self.hidden_size)\n",
    "        \n",
    "    def forward(self, x, init_states=None):\n",
    "        '''\n",
    "        x.shape is (batch_size, seq_length, feature_size)\n",
    "        recurrent_activation -> sigmoid\n",
    "        activation -> tanh\n",
    "        '''\n",
    "        if self.batch_first is True:\n",
    "            batch_size, seq_length, features_size = x.size()\n",
    "        else:\n",
    "            seq_length, batch_size, features_size = x.size()\n",
    "\n",
    "        hidden_seq = []\n",
    "        if init_states is None:\n",
    "            h_t = torch.zeros(batch_size, self.hidden_size)  # hidden state (output)\n",
    "            c_t = torch.zeros(batch_size, self.hidden_size)  # cell state\n",
    "        else:\n",
    "            # for now we ignore the fact that in PyTorch you can stack multiple RNNs\n",
    "            # so we take only the first elements of the init_states tuple init_states[0][0], init_states[1][0]\n",
    "            h_t, c_t = init_states\n",
    "            h_t = h_t[0]\n",
    "            c_t = c_t[0]\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            # get features from the t-th element in seq, for all entries in the batch\n",
    "            x_t = x[:, t, :]\n",
    "            \n",
    "            # Concatenate input and hidden state\n",
    "            v_t = torch.cat((h_t, x_t), dim=1)\n",
    "\n",
    "            # match qubit dimension\n",
    "            y_t = self.clayer_in(v_t)\n",
    "\n",
    "            f_t = torch.sigmoid(self.clayer_out(self.VQC['forget'](y_t)))  # forget block\n",
    "            i_t = torch.sigmoid(self.clayer_out(self.VQC['input'](y_t)))  # input block\n",
    "            g_t = torch.tanh(self.clayer_out(self.VQC['update'](y_t)))  # update block\n",
    "            o_t = torch.sigmoid(self.clayer_out(self.VQC['output'](y_t))) # output block\n",
    "\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        return hidden_seq, (h_t, c_t)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the possible tags: determinant, noun, verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}  # Get index for every part of speech\n",
    "ix_to_tag = {i:k for k,i in tag_to_ix.items()} #Reverse situation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below tokenizes the sentence and matches the label to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix): \n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long) #transform to tensors to work with in train func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n",
      "Entities: {0: 'DET', 1: 'NN', 2: 'V'}\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    # Tags are: DET - determiner; NN - noun; V - verb\n",
    "    # For example, the word \"The\" is a determiner\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "word_to_ix = {}\n",
    "\n",
    "# For each words-list (sentence) and tags-list in each tuple of training_data\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:  # word has not been assigned an index yet\n",
    "            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index\n",
    "\n",
    "print(f\"Vocabulary: {word_to_ix}\")\n",
    "print(f\"Entities: {ix_to_tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to pass the two sequences through the LSTM, which will output the hidden array of vectors [h_0, h_1, h_2, h_3, h_4], one for each word. A dense layer “head” is attached to the LSTM’s outputs to calculate the probability that each word may be a determinant, noun or verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, n_qubits=0):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        if n_qubits > 0: #see the kind of function\n",
    "            print(\"Tagger will use Quantum LSTM\")\n",
    "            self.lstm = QLSTM(embedding_dim, hidden_dim, n_qubits=n_qubits)\n",
    "        else:\n",
    "            print(\"Tagger will use Classical LSTM\")\n",
    "            self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        \n",
    "    def forward(self, sentence): #going to the next word\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_logits = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_logits, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 8\n",
    "hidden_dim = 6\n",
    "n_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagger will use Classical LSTM\n"
     ]
    }
   ],
   "source": [
    "model_classical = LSTMTagger(embedding_dim, \n",
    "                        hidden_dim, \n",
    "                        vocab_size=len(word_to_ix), \n",
    "                        tagset_size=len(tag_to_ix), \n",
    "                        n_qubits=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the example from the PyTorch website, we train the two networks (classical and quantum LSTM) for 300 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, n_epochs):\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'acc': []\n",
    "    }\n",
    "    for epoch in range(n_epochs):\n",
    "        losses = []\n",
    "        preds = []\n",
    "        targets = []\n",
    "        for sentence, tags in training_data:\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "            sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "            labels = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_scores = model(sentence_in)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = loss_function(tag_scores, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(float(loss))\n",
    "            \n",
    "            probs = torch.softmax(tag_scores, dim=-1)\n",
    "            preds.append(probs.argmax(dim=-1))\n",
    "            targets.append(labels)\n",
    "\n",
    "        avg_loss = np.mean(losses)\n",
    "        history['loss'].append(avg_loss)\n",
    "        \n",
    "        preds = torch.cat(preds)\n",
    "        targets = torch.cat(targets)\n",
    "        corrects = (preds == targets)\n",
    "        accuracy = corrects.sum().float() / float(targets.size(0) )\n",
    "        history['acc'].append(accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} / {n_epochs}: Loss = {avg_loss:.3f} Acc = {accuracy:.2f}\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 300: Loss = 1.107 Acc = 0.44\n",
      "Epoch 2 / 300: Loss = 1.103 Acc = 0.44\n",
      "Epoch 3 / 300: Loss = 1.098 Acc = 0.44\n",
      "Epoch 4 / 300: Loss = 1.094 Acc = 0.44\n",
      "Epoch 5 / 300: Loss = 1.091 Acc = 0.44\n",
      "Epoch 6 / 300: Loss = 1.087 Acc = 0.44\n",
      "Epoch 7 / 300: Loss = 1.084 Acc = 0.44\n",
      "Epoch 8 / 300: Loss = 1.081 Acc = 0.44\n",
      "Epoch 9 / 300: Loss = 1.078 Acc = 0.44\n",
      "Epoch 10 / 300: Loss = 1.075 Acc = 0.44\n",
      "Epoch 11 / 300: Loss = 1.072 Acc = 0.44\n",
      "Epoch 12 / 300: Loss = 1.070 Acc = 0.44\n",
      "Epoch 13 / 300: Loss = 1.067 Acc = 0.44\n",
      "Epoch 14 / 300: Loss = 1.065 Acc = 0.44\n",
      "Epoch 15 / 300: Loss = 1.063 Acc = 0.44\n",
      "Epoch 16 / 300: Loss = 1.060 Acc = 0.44\n",
      "Epoch 17 / 300: Loss = 1.058 Acc = 0.44\n",
      "Epoch 18 / 300: Loss = 1.056 Acc = 0.44\n",
      "Epoch 19 / 300: Loss = 1.054 Acc = 0.44\n",
      "Epoch 20 / 300: Loss = 1.052 Acc = 0.44\n",
      "Epoch 21 / 300: Loss = 1.050 Acc = 0.44\n",
      "Epoch 22 / 300: Loss = 1.047 Acc = 0.44\n",
      "Epoch 23 / 300: Loss = 1.045 Acc = 0.44\n",
      "Epoch 24 / 300: Loss = 1.043 Acc = 0.44\n",
      "Epoch 25 / 300: Loss = 1.041 Acc = 0.44\n",
      "Epoch 26 / 300: Loss = 1.039 Acc = 0.44\n",
      "Epoch 27 / 300: Loss = 1.037 Acc = 0.44\n",
      "Epoch 28 / 300: Loss = 1.035 Acc = 0.44\n",
      "Epoch 29 / 300: Loss = 1.032 Acc = 0.44\n",
      "Epoch 30 / 300: Loss = 1.030 Acc = 0.44\n",
      "Epoch 31 / 300: Loss = 1.028 Acc = 0.44\n",
      "Epoch 32 / 300: Loss = 1.026 Acc = 0.44\n",
      "Epoch 33 / 300: Loss = 1.024 Acc = 0.44\n",
      "Epoch 34 / 300: Loss = 1.021 Acc = 0.44\n",
      "Epoch 35 / 300: Loss = 1.019 Acc = 0.44\n",
      "Epoch 36 / 300: Loss = 1.017 Acc = 0.44\n",
      "Epoch 37 / 300: Loss = 1.014 Acc = 0.44\n",
      "Epoch 38 / 300: Loss = 1.012 Acc = 0.44\n",
      "Epoch 39 / 300: Loss = 1.009 Acc = 0.44\n",
      "Epoch 40 / 300: Loss = 1.007 Acc = 0.44\n",
      "Epoch 41 / 300: Loss = 1.004 Acc = 0.44\n",
      "Epoch 42 / 300: Loss = 1.002 Acc = 0.44\n",
      "Epoch 43 / 300: Loss = 0.999 Acc = 0.44\n",
      "Epoch 44 / 300: Loss = 0.997 Acc = 0.44\n",
      "Epoch 45 / 300: Loss = 0.994 Acc = 0.44\n",
      "Epoch 46 / 300: Loss = 0.991 Acc = 0.44\n",
      "Epoch 47 / 300: Loss = 0.988 Acc = 0.44\n",
      "Epoch 48 / 300: Loss = 0.985 Acc = 0.44\n",
      "Epoch 49 / 300: Loss = 0.982 Acc = 0.44\n",
      "Epoch 50 / 300: Loss = 0.979 Acc = 0.44\n",
      "Epoch 51 / 300: Loss = 0.976 Acc = 0.44\n",
      "Epoch 52 / 300: Loss = 0.973 Acc = 0.44\n",
      "Epoch 53 / 300: Loss = 0.970 Acc = 0.44\n",
      "Epoch 54 / 300: Loss = 0.966 Acc = 0.44\n",
      "Epoch 55 / 300: Loss = 0.963 Acc = 0.44\n",
      "Epoch 56 / 300: Loss = 0.959 Acc = 0.56\n",
      "Epoch 57 / 300: Loss = 0.956 Acc = 0.56\n",
      "Epoch 58 / 300: Loss = 0.952 Acc = 0.56\n",
      "Epoch 59 / 300: Loss = 0.948 Acc = 0.56\n",
      "Epoch 60 / 300: Loss = 0.944 Acc = 0.56\n",
      "Epoch 61 / 300: Loss = 0.940 Acc = 0.56\n",
      "Epoch 62 / 300: Loss = 0.936 Acc = 0.56\n",
      "Epoch 63 / 300: Loss = 0.932 Acc = 0.56\n",
      "Epoch 64 / 300: Loss = 0.928 Acc = 0.56\n",
      "Epoch 65 / 300: Loss = 0.923 Acc = 0.56\n",
      "Epoch 66 / 300: Loss = 0.919 Acc = 0.56\n",
      "Epoch 67 / 300: Loss = 0.914 Acc = 0.67\n",
      "Epoch 68 / 300: Loss = 0.909 Acc = 0.67\n",
      "Epoch 69 / 300: Loss = 0.904 Acc = 0.67\n",
      "Epoch 70 / 300: Loss = 0.899 Acc = 0.67\n",
      "Epoch 71 / 300: Loss = 0.894 Acc = 0.78\n",
      "Epoch 72 / 300: Loss = 0.888 Acc = 0.78\n",
      "Epoch 73 / 300: Loss = 0.882 Acc = 0.78\n",
      "Epoch 74 / 300: Loss = 0.877 Acc = 0.78\n",
      "Epoch 75 / 300: Loss = 0.871 Acc = 0.78\n",
      "Epoch 76 / 300: Loss = 0.864 Acc = 0.78\n",
      "Epoch 77 / 300: Loss = 0.858 Acc = 0.78\n",
      "Epoch 78 / 300: Loss = 0.852 Acc = 0.78\n",
      "Epoch 79 / 300: Loss = 0.845 Acc = 0.78\n",
      "Epoch 80 / 300: Loss = 0.838 Acc = 0.78\n",
      "Epoch 81 / 300: Loss = 0.831 Acc = 0.78\n",
      "Epoch 82 / 300: Loss = 0.825 Acc = 0.78\n",
      "Epoch 83 / 300: Loss = 0.818 Acc = 0.78\n",
      "Epoch 84 / 300: Loss = 0.810 Acc = 0.78\n",
      "Epoch 85 / 300: Loss = 0.803 Acc = 0.78\n",
      "Epoch 86 / 300: Loss = 0.796 Acc = 0.78\n",
      "Epoch 87 / 300: Loss = 0.789 Acc = 0.78\n",
      "Epoch 88 / 300: Loss = 0.782 Acc = 0.78\n",
      "Epoch 89 / 300: Loss = 0.775 Acc = 0.78\n",
      "Epoch 90 / 300: Loss = 0.767 Acc = 0.78\n",
      "Epoch 91 / 300: Loss = 0.760 Acc = 0.78\n",
      "Epoch 92 / 300: Loss = 0.753 Acc = 0.78\n",
      "Epoch 93 / 300: Loss = 0.745 Acc = 0.78\n",
      "Epoch 94 / 300: Loss = 0.738 Acc = 0.78\n",
      "Epoch 95 / 300: Loss = 0.731 Acc = 0.78\n",
      "Epoch 96 / 300: Loss = 0.723 Acc = 0.78\n",
      "Epoch 97 / 300: Loss = 0.716 Acc = 0.78\n",
      "Epoch 98 / 300: Loss = 0.708 Acc = 0.78\n",
      "Epoch 99 / 300: Loss = 0.701 Acc = 0.78\n",
      "Epoch 100 / 300: Loss = 0.694 Acc = 0.78\n",
      "Epoch 101 / 300: Loss = 0.686 Acc = 0.78\n",
      "Epoch 102 / 300: Loss = 0.679 Acc = 0.78\n",
      "Epoch 103 / 300: Loss = 0.671 Acc = 0.78\n",
      "Epoch 104 / 300: Loss = 0.664 Acc = 0.78\n",
      "Epoch 105 / 300: Loss = 0.656 Acc = 0.78\n",
      "Epoch 106 / 300: Loss = 0.649 Acc = 0.78\n",
      "Epoch 107 / 300: Loss = 0.641 Acc = 0.78\n",
      "Epoch 108 / 300: Loss = 0.634 Acc = 0.78\n",
      "Epoch 109 / 300: Loss = 0.626 Acc = 0.78\n",
      "Epoch 110 / 300: Loss = 0.619 Acc = 0.78\n",
      "Epoch 111 / 300: Loss = 0.611 Acc = 0.89\n",
      "Epoch 112 / 300: Loss = 0.604 Acc = 0.89\n",
      "Epoch 113 / 300: Loss = 0.596 Acc = 0.89\n",
      "Epoch 114 / 300: Loss = 0.589 Acc = 0.89\n",
      "Epoch 115 / 300: Loss = 0.581 Acc = 0.89\n",
      "Epoch 116 / 300: Loss = 0.574 Acc = 0.89\n",
      "Epoch 117 / 300: Loss = 0.566 Acc = 0.89\n",
      "Epoch 118 / 300: Loss = 0.558 Acc = 0.89\n",
      "Epoch 119 / 300: Loss = 0.551 Acc = 0.89\n",
      "Epoch 120 / 300: Loss = 0.543 Acc = 0.89\n",
      "Epoch 121 / 300: Loss = 0.536 Acc = 0.89\n",
      "Epoch 122 / 300: Loss = 0.528 Acc = 0.89\n",
      "Epoch 123 / 300: Loss = 0.521 Acc = 0.89\n",
      "Epoch 124 / 300: Loss = 0.513 Acc = 0.89\n",
      "Epoch 125 / 300: Loss = 0.506 Acc = 0.89\n",
      "Epoch 126 / 300: Loss = 0.498 Acc = 0.89\n",
      "Epoch 127 / 300: Loss = 0.491 Acc = 0.89\n",
      "Epoch 128 / 300: Loss = 0.483 Acc = 0.89\n",
      "Epoch 129 / 300: Loss = 0.476 Acc = 0.89\n",
      "Epoch 130 / 300: Loss = 0.469 Acc = 0.89\n",
      "Epoch 131 / 300: Loss = 0.461 Acc = 0.89\n",
      "Epoch 132 / 300: Loss = 0.454 Acc = 0.89\n",
      "Epoch 133 / 300: Loss = 0.447 Acc = 0.89\n",
      "Epoch 134 / 300: Loss = 0.439 Acc = 0.89\n",
      "Epoch 135 / 300: Loss = 0.432 Acc = 0.89\n",
      "Epoch 136 / 300: Loss = 0.425 Acc = 0.89\n",
      "Epoch 137 / 300: Loss = 0.418 Acc = 0.89\n",
      "Epoch 138 / 300: Loss = 0.410 Acc = 0.89\n",
      "Epoch 139 / 300: Loss = 0.403 Acc = 0.89\n",
      "Epoch 140 / 300: Loss = 0.396 Acc = 0.89\n",
      "Epoch 141 / 300: Loss = 0.389 Acc = 1.00\n",
      "Epoch 142 / 300: Loss = 0.382 Acc = 1.00\n",
      "Epoch 143 / 300: Loss = 0.375 Acc = 1.00\n",
      "Epoch 144 / 300: Loss = 0.368 Acc = 1.00\n",
      "Epoch 145 / 300: Loss = 0.362 Acc = 1.00\n",
      "Epoch 146 / 300: Loss = 0.355 Acc = 1.00\n",
      "Epoch 147 / 300: Loss = 0.348 Acc = 1.00\n",
      "Epoch 148 / 300: Loss = 0.342 Acc = 1.00\n",
      "Epoch 149 / 300: Loss = 0.335 Acc = 1.00\n",
      "Epoch 150 / 300: Loss = 0.329 Acc = 1.00\n",
      "Epoch 151 / 300: Loss = 0.322 Acc = 1.00\n",
      "Epoch 152 / 300: Loss = 0.316 Acc = 1.00\n",
      "Epoch 153 / 300: Loss = 0.310 Acc = 1.00\n",
      "Epoch 154 / 300: Loss = 0.304 Acc = 1.00\n",
      "Epoch 155 / 300: Loss = 0.298 Acc = 1.00\n",
      "Epoch 156 / 300: Loss = 0.292 Acc = 1.00\n",
      "Epoch 157 / 300: Loss = 0.286 Acc = 1.00\n",
      "Epoch 158 / 300: Loss = 0.280 Acc = 1.00\n",
      "Epoch 159 / 300: Loss = 0.275 Acc = 1.00\n",
      "Epoch 160 / 300: Loss = 0.269 Acc = 1.00\n",
      "Epoch 161 / 300: Loss = 0.264 Acc = 1.00\n",
      "Epoch 162 / 300: Loss = 0.258 Acc = 1.00\n",
      "Epoch 163 / 300: Loss = 0.253 Acc = 1.00\n",
      "Epoch 164 / 300: Loss = 0.248 Acc = 1.00\n",
      "Epoch 165 / 300: Loss = 0.243 Acc = 1.00\n",
      "Epoch 166 / 300: Loss = 0.238 Acc = 1.00\n",
      "Epoch 167 / 300: Loss = 0.233 Acc = 1.00\n",
      "Epoch 168 / 300: Loss = 0.228 Acc = 1.00\n",
      "Epoch 169 / 300: Loss = 0.224 Acc = 1.00\n",
      "Epoch 170 / 300: Loss = 0.219 Acc = 1.00\n",
      "Epoch 171 / 300: Loss = 0.215 Acc = 1.00\n",
      "Epoch 172 / 300: Loss = 0.210 Acc = 1.00\n",
      "Epoch 173 / 300: Loss = 0.206 Acc = 1.00\n",
      "Epoch 174 / 300: Loss = 0.202 Acc = 1.00\n",
      "Epoch 175 / 300: Loss = 0.198 Acc = 1.00\n",
      "Epoch 176 / 300: Loss = 0.194 Acc = 1.00\n",
      "Epoch 177 / 300: Loss = 0.190 Acc = 1.00\n",
      "Epoch 178 / 300: Loss = 0.187 Acc = 1.00\n",
      "Epoch 179 / 300: Loss = 0.183 Acc = 1.00\n",
      "Epoch 180 / 300: Loss = 0.179 Acc = 1.00\n",
      "Epoch 181 / 300: Loss = 0.176 Acc = 1.00\n",
      "Epoch 182 / 300: Loss = 0.172 Acc = 1.00\n",
      "Epoch 183 / 300: Loss = 0.169 Acc = 1.00\n",
      "Epoch 184 / 300: Loss = 0.166 Acc = 1.00\n",
      "Epoch 185 / 300: Loss = 0.163 Acc = 1.00\n",
      "Epoch 186 / 300: Loss = 0.160 Acc = 1.00\n",
      "Epoch 187 / 300: Loss = 0.157 Acc = 1.00\n",
      "Epoch 188 / 300: Loss = 0.154 Acc = 1.00\n",
      "Epoch 189 / 300: Loss = 0.151 Acc = 1.00\n",
      "Epoch 190 / 300: Loss = 0.148 Acc = 1.00\n",
      "Epoch 191 / 300: Loss = 0.145 Acc = 1.00\n",
      "Epoch 192 / 300: Loss = 0.143 Acc = 1.00\n",
      "Epoch 193 / 300: Loss = 0.140 Acc = 1.00\n",
      "Epoch 194 / 300: Loss = 0.137 Acc = 1.00\n",
      "Epoch 195 / 300: Loss = 0.135 Acc = 1.00\n",
      "Epoch 196 / 300: Loss = 0.132 Acc = 1.00\n",
      "Epoch 197 / 300: Loss = 0.130 Acc = 1.00\n",
      "Epoch 198 / 300: Loss = 0.128 Acc = 1.00\n",
      "Epoch 199 / 300: Loss = 0.126 Acc = 1.00\n",
      "Epoch 200 / 300: Loss = 0.123 Acc = 1.00\n",
      "Epoch 201 / 300: Loss = 0.121 Acc = 1.00\n",
      "Epoch 202 / 300: Loss = 0.119 Acc = 1.00\n",
      "Epoch 203 / 300: Loss = 0.117 Acc = 1.00\n",
      "Epoch 204 / 300: Loss = 0.115 Acc = 1.00\n",
      "Epoch 205 / 300: Loss = 0.113 Acc = 1.00\n",
      "Epoch 206 / 300: Loss = 0.111 Acc = 1.00\n",
      "Epoch 207 / 300: Loss = 0.109 Acc = 1.00\n",
      "Epoch 208 / 300: Loss = 0.108 Acc = 1.00\n",
      "Epoch 209 / 300: Loss = 0.106 Acc = 1.00\n",
      "Epoch 210 / 300: Loss = 0.104 Acc = 1.00\n",
      "Epoch 211 / 300: Loss = 0.102 Acc = 1.00\n",
      "Epoch 212 / 300: Loss = 0.101 Acc = 1.00\n",
      "Epoch 213 / 300: Loss = 0.099 Acc = 1.00\n",
      "Epoch 214 / 300: Loss = 0.098 Acc = 1.00\n",
      "Epoch 215 / 300: Loss = 0.096 Acc = 1.00\n",
      "Epoch 216 / 300: Loss = 0.095 Acc = 1.00\n",
      "Epoch 217 / 300: Loss = 0.093 Acc = 1.00\n",
      "Epoch 218 / 300: Loss = 0.092 Acc = 1.00\n",
      "Epoch 219 / 300: Loss = 0.090 Acc = 1.00\n",
      "Epoch 220 / 300: Loss = 0.089 Acc = 1.00\n",
      "Epoch 221 / 300: Loss = 0.088 Acc = 1.00\n",
      "Epoch 222 / 300: Loss = 0.086 Acc = 1.00\n",
      "Epoch 223 / 300: Loss = 0.085 Acc = 1.00\n",
      "Epoch 224 / 300: Loss = 0.084 Acc = 1.00\n",
      "Epoch 225 / 300: Loss = 0.083 Acc = 1.00\n",
      "Epoch 226 / 300: Loss = 0.082 Acc = 1.00\n",
      "Epoch 227 / 300: Loss = 0.080 Acc = 1.00\n",
      "Epoch 228 / 300: Loss = 0.079 Acc = 1.00\n",
      "Epoch 229 / 300: Loss = 0.078 Acc = 1.00\n",
      "Epoch 230 / 300: Loss = 0.077 Acc = 1.00\n",
      "Epoch 231 / 300: Loss = 0.076 Acc = 1.00\n",
      "Epoch 232 / 300: Loss = 0.075 Acc = 1.00\n",
      "Epoch 233 / 300: Loss = 0.074 Acc = 1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 234 / 300: Loss = 0.073 Acc = 1.00\n",
      "Epoch 235 / 300: Loss = 0.072 Acc = 1.00\n",
      "Epoch 236 / 300: Loss = 0.071 Acc = 1.00\n",
      "Epoch 237 / 300: Loss = 0.070 Acc = 1.00\n",
      "Epoch 238 / 300: Loss = 0.070 Acc = 1.00\n",
      "Epoch 239 / 300: Loss = 0.069 Acc = 1.00\n",
      "Epoch 240 / 300: Loss = 0.068 Acc = 1.00\n",
      "Epoch 241 / 300: Loss = 0.067 Acc = 1.00\n",
      "Epoch 242 / 300: Loss = 0.066 Acc = 1.00\n",
      "Epoch 243 / 300: Loss = 0.065 Acc = 1.00\n",
      "Epoch 244 / 300: Loss = 0.065 Acc = 1.00\n",
      "Epoch 245 / 300: Loss = 0.064 Acc = 1.00\n",
      "Epoch 246 / 300: Loss = 0.063 Acc = 1.00\n",
      "Epoch 247 / 300: Loss = 0.062 Acc = 1.00\n",
      "Epoch 248 / 300: Loss = 0.062 Acc = 1.00\n",
      "Epoch 249 / 300: Loss = 0.061 Acc = 1.00\n",
      "Epoch 250 / 300: Loss = 0.060 Acc = 1.00\n",
      "Epoch 251 / 300: Loss = 0.060 Acc = 1.00\n",
      "Epoch 252 / 300: Loss = 0.059 Acc = 1.00\n",
      "Epoch 253 / 300: Loss = 0.058 Acc = 1.00\n",
      "Epoch 254 / 300: Loss = 0.058 Acc = 1.00\n",
      "Epoch 255 / 300: Loss = 0.057 Acc = 1.00\n",
      "Epoch 256 / 300: Loss = 0.056 Acc = 1.00\n",
      "Epoch 257 / 300: Loss = 0.056 Acc = 1.00\n",
      "Epoch 258 / 300: Loss = 0.055 Acc = 1.00\n",
      "Epoch 259 / 300: Loss = 0.055 Acc = 1.00\n",
      "Epoch 260 / 300: Loss = 0.054 Acc = 1.00\n",
      "Epoch 261 / 300: Loss = 0.054 Acc = 1.00\n",
      "Epoch 262 / 300: Loss = 0.053 Acc = 1.00\n",
      "Epoch 263 / 300: Loss = 0.052 Acc = 1.00\n",
      "Epoch 264 / 300: Loss = 0.052 Acc = 1.00\n",
      "Epoch 265 / 300: Loss = 0.051 Acc = 1.00\n",
      "Epoch 266 / 300: Loss = 0.051 Acc = 1.00\n",
      "Epoch 267 / 300: Loss = 0.050 Acc = 1.00\n",
      "Epoch 268 / 300: Loss = 0.050 Acc = 1.00\n",
      "Epoch 269 / 300: Loss = 0.049 Acc = 1.00\n",
      "Epoch 270 / 300: Loss = 0.049 Acc = 1.00\n",
      "Epoch 271 / 300: Loss = 0.049 Acc = 1.00\n",
      "Epoch 272 / 300: Loss = 0.048 Acc = 1.00\n",
      "Epoch 273 / 300: Loss = 0.048 Acc = 1.00\n",
      "Epoch 274 / 300: Loss = 0.047 Acc = 1.00\n",
      "Epoch 275 / 300: Loss = 0.047 Acc = 1.00\n",
      "Epoch 276 / 300: Loss = 0.046 Acc = 1.00\n",
      "Epoch 277 / 300: Loss = 0.046 Acc = 1.00\n",
      "Epoch 278 / 300: Loss = 0.045 Acc = 1.00\n",
      "Epoch 279 / 300: Loss = 0.045 Acc = 1.00\n",
      "Epoch 280 / 300: Loss = 0.045 Acc = 1.00\n",
      "Epoch 281 / 300: Loss = 0.044 Acc = 1.00\n",
      "Epoch 282 / 300: Loss = 0.044 Acc = 1.00\n",
      "Epoch 283 / 300: Loss = 0.043 Acc = 1.00\n",
      "Epoch 284 / 300: Loss = 0.043 Acc = 1.00\n",
      "Epoch 285 / 300: Loss = 0.043 Acc = 1.00\n",
      "Epoch 286 / 300: Loss = 0.042 Acc = 1.00\n",
      "Epoch 287 / 300: Loss = 0.042 Acc = 1.00\n",
      "Epoch 288 / 300: Loss = 0.042 Acc = 1.00\n",
      "Epoch 289 / 300: Loss = 0.041 Acc = 1.00\n",
      "Epoch 290 / 300: Loss = 0.041 Acc = 1.00\n",
      "Epoch 291 / 300: Loss = 0.041 Acc = 1.00\n",
      "Epoch 292 / 300: Loss = 0.040 Acc = 1.00\n",
      "Epoch 293 / 300: Loss = 0.040 Acc = 1.00\n",
      "Epoch 294 / 300: Loss = 0.040 Acc = 1.00\n",
      "Epoch 295 / 300: Loss = 0.039 Acc = 1.00\n",
      "Epoch 296 / 300: Loss = 0.039 Acc = 1.00\n",
      "Epoch 297 / 300: Loss = 0.039 Acc = 1.00\n",
      "Epoch 298 / 300: Loss = 0.038 Acc = 1.00\n",
      "Epoch 299 / 300: Loss = 0.038 Acc = 1.00\n",
      "Epoch 300 / 300: Loss = 0.038 Acc = 1.00\n"
     ]
    }
   ],
   "source": [
    "history_classical = train(model_classical, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(model):\n",
    "    with torch.no_grad():\n",
    "        input_sentence = training_data[0][0]\n",
    "        labels = training_data[0][1]\n",
    "        inputs = prepare_sequence(input_sentence, word_to_ix)\n",
    "        tag_scores = model(inputs)\n",
    "\n",
    "        tag_ids = torch.argmax(tag_scores, dim=1).numpy()\n",
    "        tag_labels = [ix_to_tag[k] for k in tag_ids]\n",
    "        print(f\"Sentence:  {input_sentence}\")\n",
    "        print(f\"Labels:    {labels}\")\n",
    "        print(f\"Predicted: {tag_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  ['The', 'dog', 'ate', 'the', 'apple']\n",
      "Labels:    ['DET', 'NN', 'V', 'DET', 'NN']\n",
      "Predicted: ['DET', 'NN', 'V', 'DET', 'NN']\n"
     ]
    }
   ],
   "source": [
    "print_result(model_classical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagger will use Quantum LSTM\n",
      "weight_shapes = (n_qlayers, n_qubits) = (1, 4)\n"
     ]
    }
   ],
   "source": [
    "n_qubits = 4\n",
    "\n",
    "model_quantum = LSTMTagger(embedding_dim, \n",
    "                        hidden_dim, \n",
    "                        vocab_size=len(word_to_ix), \n",
    "                        tagset_size=len(tag_to_ix), \n",
    "                        n_qubits=n_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 300: Loss = 1.105 Acc = 0.44\n",
      "Epoch 2 / 300: Loss = 1.099 Acc = 0.44\n",
      "Epoch 3 / 300: Loss = 1.093 Acc = 0.44\n",
      "Epoch 4 / 300: Loss = 1.088 Acc = 0.44\n",
      "Epoch 5 / 300: Loss = 1.084 Acc = 0.44\n",
      "Epoch 6 / 300: Loss = 1.080 Acc = 0.44\n",
      "Epoch 7 / 300: Loss = 1.076 Acc = 0.44\n",
      "Epoch 8 / 300: Loss = 1.073 Acc = 0.44\n",
      "Epoch 9 / 300: Loss = 1.070 Acc = 0.44\n",
      "Epoch 10 / 300: Loss = 1.067 Acc = 0.44\n",
      "Epoch 11 / 300: Loss = 1.064 Acc = 0.44\n",
      "Epoch 12 / 300: Loss = 1.061 Acc = 0.44\n",
      "Epoch 13 / 300: Loss = 1.058 Acc = 0.44\n",
      "Epoch 14 / 300: Loss = 1.056 Acc = 0.44\n",
      "Epoch 15 / 300: Loss = 1.053 Acc = 0.44\n",
      "Epoch 16 / 300: Loss = 1.050 Acc = 0.44\n",
      "Epoch 17 / 300: Loss = 1.048 Acc = 0.44\n",
      "Epoch 18 / 300: Loss = 1.045 Acc = 0.44\n",
      "Epoch 19 / 300: Loss = 1.042 Acc = 0.44\n",
      "Epoch 20 / 300: Loss = 1.039 Acc = 0.44\n",
      "Epoch 21 / 300: Loss = 1.037 Acc = 0.44\n",
      "Epoch 22 / 300: Loss = 1.033 Acc = 0.44\n",
      "Epoch 23 / 300: Loss = 1.030 Acc = 0.44\n",
      "Epoch 24 / 300: Loss = 1.027 Acc = 0.44\n",
      "Epoch 25 / 300: Loss = 1.024 Acc = 0.44\n",
      "Epoch 26 / 300: Loss = 1.020 Acc = 0.44\n",
      "Epoch 27 / 300: Loss = 1.016 Acc = 0.44\n",
      "Epoch 28 / 300: Loss = 1.012 Acc = 0.44\n",
      "Epoch 29 / 300: Loss = 1.008 Acc = 0.44\n",
      "Epoch 30 / 300: Loss = 1.004 Acc = 0.44\n",
      "Epoch 31 / 300: Loss = 1.000 Acc = 0.44\n",
      "Epoch 32 / 300: Loss = 0.995 Acc = 0.44\n",
      "Epoch 33 / 300: Loss = 0.990 Acc = 0.44\n",
      "Epoch 34 / 300: Loss = 0.985 Acc = 0.44\n",
      "Epoch 35 / 300: Loss = 0.980 Acc = 0.44\n",
      "Epoch 36 / 300: Loss = 0.974 Acc = 0.44\n",
      "Epoch 37 / 300: Loss = 0.969 Acc = 0.56\n",
      "Epoch 38 / 300: Loss = 0.963 Acc = 0.56\n",
      "Epoch 39 / 300: Loss = 0.957 Acc = 0.56\n",
      "Epoch 40 / 300: Loss = 0.951 Acc = 0.56\n",
      "Epoch 41 / 300: Loss = 0.945 Acc = 0.56\n",
      "Epoch 42 / 300: Loss = 0.939 Acc = 0.78\n",
      "Epoch 43 / 300: Loss = 0.932 Acc = 0.78\n",
      "Epoch 44 / 300: Loss = 0.926 Acc = 0.78\n",
      "Epoch 45 / 300: Loss = 0.919 Acc = 0.78\n",
      "Epoch 46 / 300: Loss = 0.912 Acc = 0.78\n",
      "Epoch 47 / 300: Loss = 0.905 Acc = 0.78\n",
      "Epoch 48 / 300: Loss = 0.899 Acc = 0.78\n",
      "Epoch 49 / 300: Loss = 0.892 Acc = 0.78\n",
      "Epoch 50 / 300: Loss = 0.884 Acc = 0.78\n",
      "Epoch 51 / 300: Loss = 0.877 Acc = 0.78\n",
      "Epoch 52 / 300: Loss = 0.870 Acc = 0.78\n",
      "Epoch 53 / 300: Loss = 0.863 Acc = 0.78\n",
      "Epoch 54 / 300: Loss = 0.855 Acc = 0.78\n",
      "Epoch 55 / 300: Loss = 0.848 Acc = 0.78\n",
      "Epoch 56 / 300: Loss = 0.840 Acc = 0.78\n",
      "Epoch 57 / 300: Loss = 0.832 Acc = 0.78\n",
      "Epoch 58 / 300: Loss = 0.824 Acc = 0.78\n",
      "Epoch 59 / 300: Loss = 0.815 Acc = 0.78\n",
      "Epoch 60 / 300: Loss = 0.807 Acc = 0.78\n",
      "Epoch 61 / 300: Loss = 0.798 Acc = 0.78\n",
      "Epoch 62 / 300: Loss = 0.789 Acc = 0.78\n",
      "Epoch 63 / 300: Loss = 0.779 Acc = 0.78\n",
      "Epoch 64 / 300: Loss = 0.770 Acc = 0.78\n",
      "Epoch 65 / 300: Loss = 0.761 Acc = 0.78\n",
      "Epoch 66 / 300: Loss = 0.752 Acc = 0.78\n",
      "Epoch 67 / 300: Loss = 0.743 Acc = 0.78\n",
      "Epoch 68 / 300: Loss = 0.734 Acc = 0.78\n",
      "Epoch 69 / 300: Loss = 0.725 Acc = 0.78\n",
      "Epoch 70 / 300: Loss = 0.717 Acc = 0.78\n",
      "Epoch 71 / 300: Loss = 0.708 Acc = 0.78\n",
      "Epoch 72 / 300: Loss = 0.700 Acc = 0.78\n",
      "Epoch 73 / 300: Loss = 0.692 Acc = 0.78\n",
      "Epoch 74 / 300: Loss = 0.685 Acc = 0.78\n",
      "Epoch 75 / 300: Loss = 0.677 Acc = 0.78\n",
      "Epoch 76 / 300: Loss = 0.670 Acc = 0.78\n",
      "Epoch 77 / 300: Loss = 0.663 Acc = 0.78\n",
      "Epoch 78 / 300: Loss = 0.656 Acc = 0.78\n",
      "Epoch 79 / 300: Loss = 0.649 Acc = 0.78\n",
      "Epoch 80 / 300: Loss = 0.642 Acc = 0.78\n",
      "Epoch 81 / 300: Loss = 0.636 Acc = 0.78\n",
      "Epoch 82 / 300: Loss = 0.629 Acc = 0.78\n",
      "Epoch 83 / 300: Loss = 0.623 Acc = 0.78\n",
      "Epoch 84 / 300: Loss = 0.617 Acc = 0.78\n",
      "Epoch 85 / 300: Loss = 0.611 Acc = 0.78\n",
      "Epoch 86 / 300: Loss = 0.605 Acc = 0.78\n",
      "Epoch 87 / 300: Loss = 0.599 Acc = 0.78\n",
      "Epoch 88 / 300: Loss = 0.593 Acc = 0.78\n",
      "Epoch 89 / 300: Loss = 0.588 Acc = 0.78\n",
      "Epoch 90 / 300: Loss = 0.582 Acc = 0.78\n",
      "Epoch 91 / 300: Loss = 0.577 Acc = 0.78\n",
      "Epoch 92 / 300: Loss = 0.572 Acc = 0.78\n",
      "Epoch 93 / 300: Loss = 0.567 Acc = 0.78\n",
      "Epoch 94 / 300: Loss = 0.562 Acc = 0.78\n",
      "Epoch 95 / 300: Loss = 0.557 Acc = 0.78\n",
      "Epoch 96 / 300: Loss = 0.552 Acc = 0.78\n",
      "Epoch 97 / 300: Loss = 0.547 Acc = 0.78\n",
      "Epoch 98 / 300: Loss = 0.542 Acc = 0.78\n",
      "Epoch 99 / 300: Loss = 0.538 Acc = 0.78\n",
      "Epoch 100 / 300: Loss = 0.533 Acc = 0.78\n",
      "Epoch 101 / 300: Loss = 0.529 Acc = 0.78\n",
      "Epoch 102 / 300: Loss = 0.524 Acc = 0.78\n",
      "Epoch 103 / 300: Loss = 0.520 Acc = 0.78\n",
      "Epoch 104 / 300: Loss = 0.516 Acc = 0.78\n",
      "Epoch 105 / 300: Loss = 0.511 Acc = 0.78\n",
      "Epoch 106 / 300: Loss = 0.507 Acc = 0.78\n",
      "Epoch 107 / 300: Loss = 0.503 Acc = 0.78\n",
      "Epoch 108 / 300: Loss = 0.499 Acc = 0.78\n",
      "Epoch 109 / 300: Loss = 0.495 Acc = 0.78\n",
      "Epoch 110 / 300: Loss = 0.491 Acc = 0.78\n",
      "Epoch 111 / 300: Loss = 0.487 Acc = 0.78\n",
      "Epoch 112 / 300: Loss = 0.483 Acc = 0.78\n",
      "Epoch 113 / 300: Loss = 0.479 Acc = 0.78\n",
      "Epoch 114 / 300: Loss = 0.476 Acc = 0.78\n",
      "Epoch 115 / 300: Loss = 0.472 Acc = 0.78\n",
      "Epoch 116 / 300: Loss = 0.468 Acc = 0.78\n",
      "Epoch 117 / 300: Loss = 0.464 Acc = 0.78\n",
      "Epoch 118 / 300: Loss = 0.461 Acc = 0.78\n",
      "Epoch 119 / 300: Loss = 0.457 Acc = 0.78\n",
      "Epoch 120 / 300: Loss = 0.453 Acc = 0.78\n",
      "Epoch 121 / 300: Loss = 0.450 Acc = 0.78\n",
      "Epoch 122 / 300: Loss = 0.446 Acc = 0.78\n",
      "Epoch 123 / 300: Loss = 0.443 Acc = 0.78\n",
      "Epoch 124 / 300: Loss = 0.439 Acc = 0.78\n",
      "Epoch 125 / 300: Loss = 0.436 Acc = 0.78\n",
      "Epoch 126 / 300: Loss = 0.432 Acc = 0.78\n",
      "Epoch 127 / 300: Loss = 0.429 Acc = 0.78\n",
      "Epoch 128 / 300: Loss = 0.426 Acc = 0.78\n",
      "Epoch 129 / 300: Loss = 0.422 Acc = 0.78\n",
      "Epoch 130 / 300: Loss = 0.419 Acc = 0.78\n",
      "Epoch 131 / 300: Loss = 0.416 Acc = 0.78\n",
      "Epoch 132 / 300: Loss = 0.412 Acc = 0.78\n",
      "Epoch 133 / 300: Loss = 0.409 Acc = 0.78\n",
      "Epoch 134 / 300: Loss = 0.406 Acc = 0.78\n",
      "Epoch 135 / 300: Loss = 0.403 Acc = 0.78\n",
      "Epoch 136 / 300: Loss = 0.400 Acc = 0.78\n",
      "Epoch 137 / 300: Loss = 0.396 Acc = 0.78\n",
      "Epoch 138 / 300: Loss = 0.393 Acc = 0.78\n",
      "Epoch 139 / 300: Loss = 0.390 Acc = 0.78\n",
      "Epoch 140 / 300: Loss = 0.387 Acc = 0.78\n",
      "Epoch 141 / 300: Loss = 0.384 Acc = 0.78\n",
      "Epoch 142 / 300: Loss = 0.381 Acc = 0.78\n",
      "Epoch 143 / 300: Loss = 0.378 Acc = 0.89\n",
      "Epoch 144 / 300: Loss = 0.375 Acc = 0.89\n",
      "Epoch 145 / 300: Loss = 0.372 Acc = 0.89\n",
      "Epoch 146 / 300: Loss = 0.369 Acc = 0.89\n",
      "Epoch 147 / 300: Loss = 0.366 Acc = 0.89\n",
      "Epoch 148 / 300: Loss = 0.363 Acc = 0.89\n",
      "Epoch 149 / 300: Loss = 0.360 Acc = 0.89\n",
      "Epoch 150 / 300: Loss = 0.357 Acc = 0.89\n",
      "Epoch 151 / 300: Loss = 0.354 Acc = 0.89\n",
      "Epoch 152 / 300: Loss = 0.352 Acc = 0.89\n",
      "Epoch 153 / 300: Loss = 0.349 Acc = 0.89\n",
      "Epoch 154 / 300: Loss = 0.346 Acc = 0.89\n",
      "Epoch 155 / 300: Loss = 0.343 Acc = 0.89\n",
      "Epoch 156 / 300: Loss = 0.340 Acc = 0.89\n",
      "Epoch 157 / 300: Loss = 0.338 Acc = 0.89\n",
      "Epoch 158 / 300: Loss = 0.335 Acc = 0.89\n",
      "Epoch 159 / 300: Loss = 0.332 Acc = 0.89\n",
      "Epoch 160 / 300: Loss = 0.329 Acc = 0.89\n",
      "Epoch 161 / 300: Loss = 0.327 Acc = 0.89\n",
      "Epoch 162 / 300: Loss = 0.324 Acc = 0.89\n",
      "Epoch 163 / 300: Loss = 0.321 Acc = 0.89\n",
      "Epoch 164 / 300: Loss = 0.319 Acc = 0.89\n",
      "Epoch 165 / 300: Loss = 0.316 Acc = 0.89\n",
      "Epoch 166 / 300: Loss = 0.313 Acc = 0.89\n",
      "Epoch 167 / 300: Loss = 0.311 Acc = 1.00\n",
      "Epoch 168 / 300: Loss = 0.308 Acc = 1.00\n",
      "Epoch 169 / 300: Loss = 0.305 Acc = 1.00\n",
      "Epoch 170 / 300: Loss = 0.303 Acc = 1.00\n",
      "Epoch 171 / 300: Loss = 0.300 Acc = 1.00\n",
      "Epoch 172 / 300: Loss = 0.298 Acc = 1.00\n",
      "Epoch 173 / 300: Loss = 0.295 Acc = 1.00\n",
      "Epoch 174 / 300: Loss = 0.293 Acc = 1.00\n",
      "Epoch 175 / 300: Loss = 0.290 Acc = 1.00\n",
      "Epoch 176 / 300: Loss = 0.288 Acc = 1.00\n",
      "Epoch 177 / 300: Loss = 0.285 Acc = 1.00\n",
      "Epoch 178 / 300: Loss = 0.283 Acc = 1.00\n",
      "Epoch 179 / 300: Loss = 0.280 Acc = 1.00\n",
      "Epoch 180 / 300: Loss = 0.278 Acc = 1.00\n",
      "Epoch 181 / 300: Loss = 0.275 Acc = 1.00\n",
      "Epoch 182 / 300: Loss = 0.273 Acc = 1.00\n",
      "Epoch 183 / 300: Loss = 0.270 Acc = 1.00\n",
      "Epoch 184 / 300: Loss = 0.268 Acc = 1.00\n",
      "Epoch 185 / 300: Loss = 0.266 Acc = 1.00\n",
      "Epoch 186 / 300: Loss = 0.263 Acc = 1.00\n",
      "Epoch 187 / 300: Loss = 0.261 Acc = 1.00\n",
      "Epoch 188 / 300: Loss = 0.259 Acc = 1.00\n",
      "Epoch 189 / 300: Loss = 0.256 Acc = 1.00\n",
      "Epoch 190 / 300: Loss = 0.254 Acc = 1.00\n",
      "Epoch 191 / 300: Loss = 0.252 Acc = 1.00\n",
      "Epoch 192 / 300: Loss = 0.249 Acc = 1.00\n",
      "Epoch 193 / 300: Loss = 0.247 Acc = 1.00\n",
      "Epoch 194 / 300: Loss = 0.245 Acc = 1.00\n",
      "Epoch 195 / 300: Loss = 0.243 Acc = 1.00\n",
      "Epoch 196 / 300: Loss = 0.240 Acc = 1.00\n",
      "Epoch 197 / 300: Loss = 0.238 Acc = 1.00\n",
      "Epoch 198 / 300: Loss = 0.236 Acc = 1.00\n",
      "Epoch 199 / 300: Loss = 0.234 Acc = 1.00\n",
      "Epoch 200 / 300: Loss = 0.232 Acc = 1.00\n",
      "Epoch 201 / 300: Loss = 0.230 Acc = 1.00\n",
      "Epoch 202 / 300: Loss = 0.227 Acc = 1.00\n",
      "Epoch 203 / 300: Loss = 0.225 Acc = 1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204 / 300: Loss = 0.223 Acc = 1.00\n",
      "Epoch 205 / 300: Loss = 0.221 Acc = 1.00\n",
      "Epoch 206 / 300: Loss = 0.219 Acc = 1.00\n",
      "Epoch 207 / 300: Loss = 0.217 Acc = 1.00\n",
      "Epoch 208 / 300: Loss = 0.215 Acc = 1.00\n",
      "Epoch 209 / 300: Loss = 0.213 Acc = 1.00\n",
      "Epoch 210 / 300: Loss = 0.211 Acc = 1.00\n",
      "Epoch 211 / 300: Loss = 0.209 Acc = 1.00\n",
      "Epoch 212 / 300: Loss = 0.207 Acc = 1.00\n",
      "Epoch 213 / 300: Loss = 0.205 Acc = 1.00\n",
      "Epoch 214 / 300: Loss = 0.203 Acc = 1.00\n",
      "Epoch 215 / 300: Loss = 0.201 Acc = 1.00\n",
      "Epoch 216 / 300: Loss = 0.200 Acc = 1.00\n",
      "Epoch 217 / 300: Loss = 0.198 Acc = 1.00\n",
      "Epoch 218 / 300: Loss = 0.196 Acc = 1.00\n",
      "Epoch 219 / 300: Loss = 0.194 Acc = 1.00\n",
      "Epoch 220 / 300: Loss = 0.192 Acc = 1.00\n",
      "Epoch 221 / 300: Loss = 0.191 Acc = 1.00\n",
      "Epoch 222 / 300: Loss = 0.189 Acc = 1.00\n",
      "Epoch 223 / 300: Loss = 0.187 Acc = 1.00\n",
      "Epoch 224 / 300: Loss = 0.185 Acc = 1.00\n",
      "Epoch 225 / 300: Loss = 0.184 Acc = 1.00\n",
      "Epoch 226 / 300: Loss = 0.182 Acc = 1.00\n",
      "Epoch 227 / 300: Loss = 0.180 Acc = 1.00\n",
      "Epoch 228 / 300: Loss = 0.179 Acc = 1.00\n",
      "Epoch 229 / 300: Loss = 0.177 Acc = 1.00\n",
      "Epoch 230 / 300: Loss = 0.175 Acc = 1.00\n",
      "Epoch 231 / 300: Loss = 0.174 Acc = 1.00\n",
      "Epoch 232 / 300: Loss = 0.172 Acc = 1.00\n",
      "Epoch 233 / 300: Loss = 0.171 Acc = 1.00\n",
      "Epoch 234 / 300: Loss = 0.169 Acc = 1.00\n",
      "Epoch 235 / 300: Loss = 0.168 Acc = 1.00\n",
      "Epoch 236 / 300: Loss = 0.166 Acc = 1.00\n",
      "Epoch 237 / 300: Loss = 0.165 Acc = 1.00\n",
      "Epoch 238 / 300: Loss = 0.163 Acc = 1.00\n",
      "Epoch 239 / 300: Loss = 0.162 Acc = 1.00\n",
      "Epoch 240 / 300: Loss = 0.160 Acc = 1.00\n",
      "Epoch 241 / 300: Loss = 0.159 Acc = 1.00\n",
      "Epoch 242 / 300: Loss = 0.157 Acc = 1.00\n",
      "Epoch 243 / 300: Loss = 0.156 Acc = 1.00\n",
      "Epoch 244 / 300: Loss = 0.155 Acc = 1.00\n",
      "Epoch 245 / 300: Loss = 0.153 Acc = 1.00\n",
      "Epoch 246 / 300: Loss = 0.152 Acc = 1.00\n",
      "Epoch 247 / 300: Loss = 0.151 Acc = 1.00\n",
      "Epoch 248 / 300: Loss = 0.149 Acc = 1.00\n",
      "Epoch 249 / 300: Loss = 0.148 Acc = 1.00\n",
      "Epoch 250 / 300: Loss = 0.147 Acc = 1.00\n",
      "Epoch 251 / 300: Loss = 0.146 Acc = 1.00\n",
      "Epoch 252 / 300: Loss = 0.144 Acc = 1.00\n",
      "Epoch 253 / 300: Loss = 0.143 Acc = 1.00\n",
      "Epoch 254 / 300: Loss = 0.142 Acc = 1.00\n",
      "Epoch 255 / 300: Loss = 0.141 Acc = 1.00\n",
      "Epoch 256 / 300: Loss = 0.139 Acc = 1.00\n",
      "Epoch 257 / 300: Loss = 0.138 Acc = 1.00\n",
      "Epoch 258 / 300: Loss = 0.137 Acc = 1.00\n",
      "Epoch 259 / 300: Loss = 0.136 Acc = 1.00\n",
      "Epoch 260 / 300: Loss = 0.135 Acc = 1.00\n",
      "Epoch 261 / 300: Loss = 0.134 Acc = 1.00\n",
      "Epoch 262 / 300: Loss = 0.133 Acc = 1.00\n",
      "Epoch 263 / 300: Loss = 0.131 Acc = 1.00\n",
      "Epoch 264 / 300: Loss = 0.130 Acc = 1.00\n",
      "Epoch 265 / 300: Loss = 0.129 Acc = 1.00\n",
      "Epoch 266 / 300: Loss = 0.128 Acc = 1.00\n",
      "Epoch 267 / 300: Loss = 0.127 Acc = 1.00\n",
      "Epoch 268 / 300: Loss = 0.126 Acc = 1.00\n",
      "Epoch 269 / 300: Loss = 0.125 Acc = 1.00\n",
      "Epoch 270 / 300: Loss = 0.124 Acc = 1.00\n",
      "Epoch 271 / 300: Loss = 0.123 Acc = 1.00\n",
      "Epoch 272 / 300: Loss = 0.122 Acc = 1.00\n",
      "Epoch 273 / 300: Loss = 0.121 Acc = 1.00\n",
      "Epoch 274 / 300: Loss = 0.120 Acc = 1.00\n",
      "Epoch 275 / 300: Loss = 0.119 Acc = 1.00\n",
      "Epoch 276 / 300: Loss = 0.118 Acc = 1.00\n",
      "Epoch 277 / 300: Loss = 0.117 Acc = 1.00\n",
      "Epoch 278 / 300: Loss = 0.117 Acc = 1.00\n",
      "Epoch 279 / 300: Loss = 0.116 Acc = 1.00\n",
      "Epoch 280 / 300: Loss = 0.115 Acc = 1.00\n",
      "Epoch 281 / 300: Loss = 0.114 Acc = 1.00\n",
      "Epoch 282 / 300: Loss = 0.113 Acc = 1.00\n",
      "Epoch 283 / 300: Loss = 0.112 Acc = 1.00\n",
      "Epoch 284 / 300: Loss = 0.111 Acc = 1.00\n",
      "Epoch 285 / 300: Loss = 0.110 Acc = 1.00\n",
      "Epoch 286 / 300: Loss = 0.110 Acc = 1.00\n",
      "Epoch 287 / 300: Loss = 0.109 Acc = 1.00\n",
      "Epoch 288 / 300: Loss = 0.108 Acc = 1.00\n",
      "Epoch 289 / 300: Loss = 0.107 Acc = 1.00\n",
      "Epoch 290 / 300: Loss = 0.106 Acc = 1.00\n",
      "Epoch 291 / 300: Loss = 0.106 Acc = 1.00\n",
      "Epoch 292 / 300: Loss = 0.105 Acc = 1.00\n",
      "Epoch 293 / 300: Loss = 0.104 Acc = 1.00\n",
      "Epoch 294 / 300: Loss = 0.103 Acc = 1.00\n",
      "Epoch 295 / 300: Loss = 0.102 Acc = 1.00\n",
      "Epoch 296 / 300: Loss = 0.102 Acc = 1.00\n",
      "Epoch 297 / 300: Loss = 0.101 Acc = 1.00\n",
      "Epoch 298 / 300: Loss = 0.100 Acc = 1.00\n",
      "Epoch 299 / 300: Loss = 0.100 Acc = 1.00\n",
      "Epoch 300 / 300: Loss = 0.099 Acc = 1.00\n"
     ]
    }
   ],
   "source": [
    "history_quantum = train(model_quantum, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  ['The', 'dog', 'ate', 'the', 'apple']\n",
      "Labels:    ['DET', 'NN', 'V', 'DET', 'NN']\n",
      "Predicted: ['DET', 'NN', 'V', 'DET', 'NN']\n"
     ]
    }
   ],
   "source": [
    "print_result(model_quantum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_history(history_classical, history_quantum):\n",
    "    loss_c = history_classical['loss']\n",
    "    acc_c = history_classical['acc']\n",
    "    loss_q = history_quantum['loss']\n",
    "    acc_q = history_quantum['acc']\n",
    "    n_epochs = max([len(loss_c), len(loss_q)])\n",
    "    x_epochs = [i for i in range(n_epochs)]\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "    \n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.plot(loss_c, label=\"Classical LSTM loss\", color='orange', linestyle='dashed')\n",
    "    ax1.plot(loss_q, label=\"Quantum LSTM loss\", color='red', linestyle='solid')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    ax2.plot(acc_c, label=\"Classical LSTM accuracy\", color='steelblue', linestyle='dashed')\n",
    "    ax2.plot(acc_q, label=\"Quantum LSTM accuracy\", color='blue', linestyle='solid')\n",
    "\n",
    "    plt.title(\"Part-of-Speech Tagger Training\")\n",
    "    plt.ylim(0., 1.1)\n",
    "    #plt.legend(loc=\"upper right\")\n",
    "    fig.legend(loc=\"upper right\", bbox_to_anchor=(1,0.8), bbox_transform=ax1.transAxes)\n",
    "\n",
    "    plt.savefig(\"pos_training.pdf\")\n",
    "    plt.savefig(\"pos_training.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history_classical, history_quantum) #comparison of two models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function decreases as a function of the training epoch, and after 300 epochs both networks are able to tag correctly the first sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
